<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://annvix.com/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://annvix.com/theme/css/custom.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://annvix.com/theme/css/fontawesome.min.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Vincent Danen" />

        <meta name="description" content="It’s been a bit since I’ve had a chance to write a blog post about something technical. As with most people in the world, I’ve been trying to dive deeper into using AI and making it make me more efficient and useful. I’ve spent quite a …
" />
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="macos, ai, llm, productivity, Macos, " />

<meta property="og:title" content="Using Obsidian with a local LLM "/>
<meta property="og:url" content="https://annvix.com/blog/using-obsidian-with-a-local-llm" />
<meta property="og:description" content="It’s been a bit since I’ve had a chance to write a blog post about something technical. As with most people in the world, I’ve been trying to dive deeper into using AI and making it make me more efficient and useful. I’ve spent quite a …" />
<meta property="og:site_name" content="Annvix" />
<meta property="og:article:author" content="Vincent Danen" />
<meta property="og:article:published_time" content="2025-05-07T20:40:00-06:00" />
<meta name="twitter:title" content="Using Obsidian with a local LLM ">
<meta name="twitter:description" content="It’s been a bit since I’ve had a chance to write a blog post about something technical. As with most people in the world, I’ve been trying to dive deeper into using AI and making it make me more efficient and useful. I’ve spent quite a …">

        <title>Using Obsidian with a local LLM  · Annvix
</title>
        <link rel="shortcut icon" href="https://annvix.com/theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="https://annvix.com/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="https://annvix.com/theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="https://annvix.com/theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="https://annvix.com/theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="https://annvix.com/theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="https://annvix.com/theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="https://annvix.com/theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="https://annvix.com/theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://annvix.com/theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="https://annvix.com/theme/images/apple-touch-icon-180x180.png" type="image/png" />
        <link href="https://annvix.com/recent.atom" type="application/atom+xml" rel="alternate" title="Annvix - Full Atom Feed" />
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5JGVD90SKJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5JGVD90SKJ');
</script>



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://annvix.com/"><span class=site-name>Annvix</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://annvix.com
                                    >Home</a>
                                </li>
                                <li ><a href="https://annvix.com/categories">Categories</a></li>
                                <li ><a href="https://annvix.com/tags">Tags</a></li>
                                <li ><a href="https://annvix.com/archives">Archives</a></li>
                                <li><form class="navbar-search" action="https://annvix.com/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://annvix.com/blog/using-obsidian-with-a-local-llm">
                Using Obsidian with a local <span class="caps">LLM</span>
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>It&#8217;s been a bit since I&#8217;ve had a chance to write a blog post about
something technical.  As with most people in the world, I&#8217;ve been trying to
dive deeper into using <span class="caps">AI</span> and making it make me more efficient and useful.
I&#8217;ve spent quite a bit of time in the last week or so with Google&#8217;s Gemini,
which is great, but I&#8217;m really interested in using <span class="caps">AI</span> to summarize or get
informations about content that I&#8217;ve stored in Obsidian, which I&#8217;ve
discussed before (see
<a href="https://annvix.com/blog/productivity-improvements-with-obsidian-omnifocus-and-johnny-decimal">Productivity improvements with Obsidian, OmniFocus and Johnny
Decimal</a>.
Because my Obsidian vault contains a lot of private information (I take a
lot of meeting notes, for example) I didn&#8217;t want to connect it to any
public&nbsp;services.</p>
<p>I couldn&#8217;t find an easy to guide to do this, so this is that guide.  It was
pretty straightforward and simple, so hopefully this will help others.
I&#8217;ll likely follow up in the future with more things that I discover and&nbsp;learn.</p>
<p>We&#8217;ll be doing two things here.  The first is installing <a href="https://github.com/zylon-ai/private-gpt">Private
<span class="caps">GPT</span></a> because my initial reading
made me think I needed it.  You don&#8217;t really, although it looks useful and
might be a nice tie-in to some other applications.  We&#8217;ll also be
installing <a href="https://ollama.com/">Ollama</a> and some Obsidian plugins to talk
directly to it, without Private <span class="caps">GPT</span> as an intermediary.  The instructions
will assume a MacOS system, although it shouldn&#8217;t be too hard to do this on
Linux as&nbsp;well.</p>
<p>First, we install&nbsp;Ollama:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span>brew<span class="w"> </span>install<span class="w"> </span>--cask<span class="w"> </span>ollama
$<span class="w"> </span>brew<span class="w"> </span>install<span class="w"> </span>ollama
</pre></div>


<p>You can use the <span class="caps">GUI</span> application, but we want to run Ollma on the
commandline and make sure it&nbsp;works:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span>ollama<span class="w"> </span>serve
</pre></div>


<p>In another window, download the models you want to&nbsp;use:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span>ollama<span class="w"> </span>pull<span class="w"> </span>llama3.1
$<span class="w"> </span>ollama<span class="w"> </span>pull<span class="w"> </span>nomic-embed-text
</pre></div>


<p>When this is done, install Private <span class="caps">GPT</span> by first cloning the git&nbsp;repository:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>~/git
$<span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/zylon-ai/private-gpt
$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>private-gpt
</pre></div>


<p>Create a python virtual environment.  If you don&#8217;t have python 3.11, you&#8217;ll
need to install it.  My Homebrew setup has 3.13, so I also needed to
install 3.11 (it will not work with&nbsp;3.13):</p>
<div class="highlight"><pre><span></span><span class="n">$</span><span class="w"> </span><span class="n">brew</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="n">python</span><span class="mf">@3.11</span>
<span class="n">$</span><span class="w"> </span><span class="n">python3</span><span class="mf">.11</span><span class="w"> </span><span class="o">-</span><span class="n">m</span><span class="w"> </span><span class="n">venv</span><span class="w"> </span><span class="p">.</span><span class="n">venv</span>
<span class="n">$</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="p">.</span><span class="n">venv</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">activate</span>
</pre></div>


<p>Install the dependencies that Private <span class="caps">GPT</span>&nbsp;needs:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span>poetry<span class="w"> </span>install<span class="w"> </span>--extras<span class="w"> </span><span class="s2">&quot;ui llms-ollama embeddings-ollama vector-stores-qdrant&quot;</span>
</pre></div>


<p>Now you can run Private <span class="caps">GPT</span>:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nv">PGPT_PROFILES</span><span class="o">=</span>ollama<span class="w"> </span>make<span class="w"> </span>run
</pre></div>


<p>It will take a bit to initialize and start, once you&nbsp;see</p>
<div class="highlight"><pre><span></span><span class="mi">15</span><span class="o">:</span><span class="mi">16</span><span class="o">:</span><span class="mf">59.694</span><span class="w"> </span><span class="o">[</span><span class="n">INFO</span><span class="w">    </span><span class="o">]</span><span class="w">             </span><span class="n">uvicorn</span><span class="o">.</span><span class="na">error</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Application</span><span class="w"> </span><span class="n">startup</span><span class="w"> </span><span class="n">complete</span><span class="o">.</span>
<span class="mi">15</span><span class="o">:</span><span class="mi">16</span><span class="o">:</span><span class="mf">59.695</span><span class="w"> </span><span class="o">[</span><span class="n">INFO</span><span class="w">    </span><span class="o">]</span><span class="w">             </span><span class="n">uvicorn</span><span class="o">.</span><span class="na">error</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Uvicorn</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">http</span><span class="o">://</span><span class="mf">0.0</span><span class="o">.</span><span class="mf">0.0</span><span class="o">:</span><span class="mi">8001</span><span class="w"> </span><span class="o">(</span><span class="n">Press</span><span class="w"> </span><span class="n">CTRL</span><span class="o">+</span><span class="n">C</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">quit</span><span class="o">)</span>
</pre></div>


<p>Now you can connect to http://localhost:8001 and use Private <span class="caps">GPT</span> through
its web&nbsp;interface.</p>
<p>But you don&#8217;t need Private <span class="caps">GPT</span> to work with Obsidian, you just need Ollama.
There are two plugins to&nbsp;install:</p>
<ul>
<li><a href="https://github.com/pfrankov/obsidian-local-gpt">Local <span class="caps">GPT</span></a></li>
<li><a href="https://github.com/pfrankov/obsidian-ai-providers">Obsidian <span class="caps">AI</span>&nbsp;Providers</a></li>
</ul>
<p>These community plugins can be installed within&nbsp;Obsidian.</p>
<h3 id="obsidian-ai-providers">Obsidian <span class="caps">AI</span> Providers<a class="headerlink" href="#obsidian-ai-providers" title="Permanent link"> </a></h3>
<p>This is where you configure the <span class="caps">AI</span> provider for Local <span class="caps">GPT</span> to use.  It&#8217;s
where I expected to configure Private <span class="caps">GPT</span> but I found that it will talk to
Ollama directly.  The configuration is pretty straightforward, there are a
number of providers you can chose from and Ollama is one of them.  Give it
a name and the <span class="caps">URL</span> should be http://127.0.0.1:11434.  Click the refresh
button next to the Model dropdown and you can select whichever model you
like that you loaded into Ollama (in my case, <code>llama3.1:latest</code>).  If you
followed the instructions for Private <span class="caps">GPT</span> and Ollama above, you can add a
second provider here for the <code>nomic-embed-text:latest</code> model as&nbsp;well.</p>
<h3 id="local-gpt">Local <span class="caps">GPT</span><a class="headerlink" href="#local-gpt" title="Permanent link"> </a></h3>
<p>There&#8217;s almost nothing to configure here at the outset; you can find some
different prompts but the defaults are fine to start with.  All you really
need to set here is the Main <span class="caps">AI</span> Provider, and point it to what you
configured in Obsidian <span class="caps">AI</span> Providers for <code>llama3.1:latest</code>.  You can also
add the Embedding <span class="caps">AI</span> Provider and point it to the <code>nomic-embed-text:latest</code>
provider.</p>
<p>Out of the box there&#8217;s no context menu so you need to use the command
palette, or set a hotkey.  I went to Obsidian&#8217;s Hotkeys configuration,
filtered on &#8220;<span class="caps">GPT</span>&#8221; and set <span class="caps">OPT</span>+<span class="caps">CMD</span>+G to <em>Local <span class="caps">GPT</span>: Show context menu</em> which
will bring up all of the available&nbsp;prompts.</p>
<p>I don&#8217;t believe the models are trained on any of the data in my Obsidian
vault, so in the context of Obsidian alone, I don&#8217;t really need Private <span class="caps">GPT</span>
&#8212; for anything outside of my vault that&#8217;s public I can use ChatGPT or
Google&#8217;s Gemini so there doesn&#8217;t appear to me to be a reason to run Private
<span class="caps">GPT</span> unless I can train it on my vault data, which is something I will
experiment with in the&nbsp;future.</p>
<p>To have Ollama run persistently in the background, start it with <code>brew
services</code>:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span>brew<span class="w"> </span>services<span class="w"> </span>start<span class="w"> </span>ollama
</pre></div>


<p>At this point, if you like, you can explore other models in Ollama, such as
the DeepSeek models and others.  The full list is available
<a href="https://ollama.com/library">here</a>.  And you can interact with your local
model using Obsidian directly using the plugins as discussed, or using
Private <span class="caps">GPT</span>.</p>
<p>I&#8217;ve experimented with it a bit and it&#8217;s pretty fast.  Of course, mileage
will vary but I&#8217;m using this on a Mac Studio M1 and it&#8217;s pretty zippy with
what I&#8217;ve tried.  I plan to fiddle with it more, and ideally train the
local model in some way with my data to get some better targeted
information across my Obsidian vault rather than just the current document
I&#8217;m working on.  Maybe this weekend&nbsp;;)</p>
<h1 id="update-20250509">Update 20250509<a class="headerlink" href="#update-20250509" title="Permanent link"> </a></h1>
<p>Spent a little more time fiddling with this.  I had initially done this
setup on an M1 Mac Studio, but I also have an M3 Mac Studio and wanted to
make use of it, so I replicated the above on the M3.  The M1 is where I use
Obsidian nearly every day, so rather than pointing to <code>http://localhost</code> I
wanted to point to <code>http://192.168...</code> which is the <span class="caps">IP</span> of the M3.  Really
simply to do in the Obsidian <span class="caps">AI</span> Providers by just changing the <span class="caps">URL</span>.  On the
other host, however, you need to chane Ollama to listen to all ports as by
default it only listens to the&nbsp;localhost.</p>
<p>This can be easily accomplished by&nbsp;using:</p>
<div class="highlight"><pre><span></span>$<span class="w"> </span>launchctl<span class="w"> </span>setenv<span class="w"> </span>OLLAMA_HOST<span class="w"> </span><span class="m">0</span>.0.0.0:11434
</pre></div>


<p>And restarting Ollama.  You&#8217;ll be able to connect to a remote (but still on
the local network) Ollama and share it across multiple systems rather than
installing it on each local system you wish to use it Providers by just
changing the <span class="caps">URL</span>.  On the other host, however, you need to chane Ollama to
listen to all ports as by default it only listens to the&nbsp;localhost.</p>
<p>The obnoxious thing is this doesn&#8217;t persist, so anytime you reboot your
system you&#8217;ll need to use the above and then <code>brew services restart
ollama</code>.  I&#8217;ve not tried to figure this out yet, but it might be something
as simple as starting a shellscript to set the environment before running
the <code>brew services</code> command, although I imagine there must be something
more&nbsp;elegant.</p>


             
 
                <p id="post-share-links">
    Share on:
      <a href="https://twitter.com/intent/tweet?text=Using%20Obsidian%20with%20a%20local%20LLM&url=https%3A//annvix.com/blog/using-obsidian-with-a-local-llm&hashtags=macos,ai,llm,productivity" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
 ❄       <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A//annvix.com/blog/using-obsidian-with-a-local-llm&title=Using%20Obsidian%20with%20a%20local%20LLM&summary=It%E2%80%99s%20been%20a%20bit%20since%20I%E2%80%99ve%20had%20a%20chance%20to%20write%20a%20blog%20post%20about%0Asomething%20technical.%20%20As%20with%20most%20people%20in%20the%20world%2C%20I%E2%80%99ve%20been%20trying%20to%0Adive%20deeper%20into%20using%20AI%20and%20making%20it%20make%20me%20more%20efficient%20and%20useful.%0AI%E2%80%99ve%20spent%20quite%20a%20%E2%80%A6&source=https%3A//annvix.com/blog/using-obsidian-with-a-local-llm" target="_blank" rel="nofollow noopener noreferrer" title="Share on LinkedIn">LinkedIn</a>

            
            







            <hr/>
<section>
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="https://annvix.com/blog/productivity-improvements-with-obsidian-omnifocus-and-johnny-decimal" title="Productivity improvements with Obsidian, OmniFocus and Johnny Decimal">Productivity improvements with Obsidian, OmniFocus and Johnny Decimal</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="https://annvix.com/blog/security-unscripted" title="Previous: Security Unscripted">Security Unscripted</a></li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2025-05-07T20:40:00-06:00">May 07, 2025</time>
            <h4>Category</h4>
            <a class="category-link" href="https://annvix.com/categories#macos-ref">Macos</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://annvix.com/tags#ai-ref">ai
                    <span class="superscript">1</span>
</a></li>
                <li><a href="https://annvix.com/tags#llm-ref">llm
                    <span class="superscript">1</span>
</a></li>
                <li><a href="https://annvix.com/tags#macos-ref">macos
                    <span class="superscript">1</span>
</a></li>
                <li><a href="https://annvix.com/tags#productivity-ref">productivity
                    <span class="superscript">2</span>
</a></li>
            </ul>
<h4>Stay in touch</h4>
<div id="sidebar-social-link">
    <a href="https://twitter.com/vdanen" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="https://github.com/vdanen" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.linkedin.com/in/vdanen/" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="LinkedIn" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%" fill="#0077b5"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg>
    </a>
    <a href="https://infosec.exchange/@vdanen" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Mastodon" role="img" viewBox="0 0 512 512" fill="#fff"><rect width="512" height="512" rx="15%"/><path d="m409 290c-5 24-43 50-85 56-86 11-137-6-137-6 3 13-4 54 70 52 31 0 58-7 58-7l2 27c-51 24-107 15-140 6-67-17-79-90-81-162v-59c0-74 49-96 49-96 50-24 180-22 222 0 0 0 49 22 49 96 0 0 1 55-7 93" fill="#3088d4"/><path d="m358 202v91h-35v-88c0-18-8-27-23-27-18 0-27 11-27 33v47h-34v-47c0-22-9-33-27-33-15 0-23 9-23 27v88h-35v-91c0-18 5-60 52-60 39 0 50 37 50 37s10-37 50-37c45 0 52 42 52 60"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>
    <div>
        Copyright &copy; 2024 Vincent Danen
    </div>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://annvix.com/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>